{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Top Repositories for Topics on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "TODO:\n",
    "- This is a basic project to try on Web Scraping:Automating the process of collecting valuable information from websites.\n",
    "- We are scraping GitHub:GitHub is a code hosting platform for version control and collaboration and has a collection of huge number of open source project repositories\n",
    "- In this project we will try to goto the topic section of GitHub, where we can find top topics that people are contributing to.We will list down the topics and topic wise we will scrape the information of the top contributor along with its repository name and link and save it as a csv file\n",
    "- Tools used- Python-for writing scripts,\n",
    "             requests-to fetch the web pages,\n",
    "             Beautiful Soup-to parser through the web page contents,\n",
    "             Pandas-for formatting our final output in csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Here are what we are going to follow:\n",
    "- We are going to scrape https://github.com/topics\n",
    "- We will get a list of topics and for each topic, we'll get topic title,topic page URL and topic descriptionl \n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page \n",
    "- For each repositorie,we'll grab the repo name. username , stars and repo URL \n",
    "- For each topic we'll create a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the list of topics from the GitHub\n",
    " -Explaination of the process:\n",
    "- use requests to download the page\n",
    "- use BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response=requests.get(topics_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "        \n",
    "    doc=BeautifulSoup(response.text,'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_doc=get_topics_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create some helper functions to parse information from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To retrieve the topic titles\n",
    "def get_topic_titles(doc):\n",
    "    selection_class='f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags=doc.find_all('p',{'class': selection_class})\n",
    "    topic_titles=[]\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(topics_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To retrieve the the descriptions of each topics:\n",
    "def get_topic_descs(doc):\n",
    "    topic_desc_tags=doc.find_all('p',{'class':'f5 color-text-secondary mb-0 mt-1'})\n",
    "    topic_descs=[]\n",
    "\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to retrieve the urls of each topic:\n",
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a',{'class':'d-flex no-underline'})\n",
    "\n",
    "    topic_urls =[]\n",
    "    base_url='https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url+tag['href']) \n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all the functions under a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response=requests.get(topics_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc=response.text \n",
    "    doc=BeautifulSoup(doc,'html.parser')\n",
    "    topics_dict ={\n",
    "        'title': get_topic_titles(doc),\n",
    "        'Description':get_topic_descs(doc),\n",
    "        'url':get_topic_urls(doc)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the top repositories in the topic from the topic page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    #download page\n",
    "    response =requests.get(topic_url)\n",
    "    #check response\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    #parse using beautiful soup\n",
    "    topic_doc=BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= get_topic_page('https://githuub.com/topics/3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tags,star_tag):\n",
    "    base_url='https://github.com'\n",
    "    \n",
    "    #returns all the required info about a repository\n",
    "    a_tags=h1_tags.find_all('a')\n",
    "    username =a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url=base_url+a_tags[1]['href']\n",
    "    stars=parse_star_count(star_tag.text.strip())\n",
    "    return username,repo_name,stars,repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str= stars_str.strip()\n",
    "    if stars_str[-1]=='k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    #Get the h1 tags containing repo title ,repo URl and user name\n",
    "    h1_selection_class='f3 color-text-secondary text-normal lh-condensed'\n",
    "    repo_tags=topic_doc.find_all('h1',{'class':h1_selection_class})\n",
    "    #get star tags\n",
    "    star_tags = topic_doc.find_all('a',{'class':'social-count float-none'})\n",
    "    #creating a dictionary to save all info:\n",
    "    topic_repos_dict={ 'usernames':[], 'repo-names':[],'stars':[],'repo_urls':[]}\n",
    "    \n",
    "    #Get repo info:\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info=get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repos_dict['usernames'].append(repo_info[0])\n",
    "        topic_repos_dict['repo-names'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_urls'].append(repo_info[3])\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def scrape_topic(topic_url,fname):\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        print(\"The file {} already exist. skipping...\".format(fname))\n",
    "        return\n",
    "    topic_df=get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(fname,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "- We have a function to get the list of topics\n",
    "- We have a function to create a CSV file for scrapped repos from a topics page\n",
    "- Let's create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print(\"Scraping list of topics from GitHub\")\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for {} '.format(row['title']))\n",
    "        scrape_topic(row['url'],row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics from GitHub\n",
      "Scraping top repositories for 3D \n",
      "The file 3D already exist. skipping...\n",
      "Scraping top repositories for Ajax \n",
      "The file Ajax already exist. skipping...\n",
      "Scraping top repositories for Algorithm \n",
      "The file Algorithm already exist. skipping...\n",
      "Scraping top repositories for Amp \n",
      "The file Amp already exist. skipping...\n",
      "Scraping top repositories for Android \n",
      "The file Android already exist. skipping...\n",
      "Scraping top repositories for Angular \n",
      "The file Angular already exist. skipping...\n",
      "Scraping top repositories for Ansible \n",
      "The file Ansible already exist. skipping...\n",
      "Scraping top repositories for API \n",
      "The file API already exist. skipping...\n",
      "Scraping top repositories for Arduino \n",
      "The file Arduino already exist. skipping...\n",
      "Scraping top repositories for ASP.NET \n",
      "The file ASP.NET already exist. skipping...\n",
      "Scraping top repositories for Atom \n",
      "The file Atom already exist. skipping...\n",
      "Scraping top repositories for Awesome Lists \n",
      "The file Awesome Lists already exist. skipping...\n",
      "Scraping top repositories for Amazon Web Services \n",
      "The file Amazon Web Services already exist. skipping...\n",
      "Scraping top repositories for Azure \n",
      "The file Azure already exist. skipping...\n",
      "Scraping top repositories for Babel \n",
      "The file Babel already exist. skipping...\n",
      "Scraping top repositories for Bash \n",
      "The file Bash already exist. skipping...\n",
      "Scraping top repositories for Bitcoin \n",
      "The file Bitcoin already exist. skipping...\n",
      "Scraping top repositories for Bootstrap \n",
      "The file Bootstrap already exist. skipping...\n",
      "Scraping top repositories for Bot \n",
      "The file Bot already exist. skipping...\n",
      "Scraping top repositories for C \n",
      "The file C already exist. skipping...\n",
      "Scraping top repositories for Chrome \n",
      "Scraping top repositories for Chrome extension \n",
      "Scraping top repositories for Command line interface \n",
      "Scraping top repositories for Clojure \n",
      "Scraping top repositories for Code quality \n",
      "Scraping top repositories for Code review \n",
      "Scraping top repositories for Compiler \n",
      "Scraping top repositories for Continuous integration \n",
      "Scraping top repositories for COVID-19 \n",
      "Scraping top repositories for C++ \n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
